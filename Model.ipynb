{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dipterv Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "smDVFjhi-uIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crossvalodation\n",
        "CV_NUM_FOLDS = 10\n",
        "CV_TRAIN_RATIO = 0.9\n",
        "\n",
        "# NCF model parameters\n",
        "NCF_EPOCHS = 100\n",
        "NCF_ES_PATIENCE = 20\n",
        "# NCF_DROPOUT = 0.2\n",
        "NCF_LR = 0.005\n",
        "NCF_BATCH_SIZE = 1024\n",
        "\n",
        "# LightGCN model parameters\n",
        "LGCN_EPOCHS = 50\n",
        "LGCN_EVAL_EPOCHS = -1 # -1 to turn off\n",
        "LGCN_N_LAYERS = 3\n",
        "LGCN_EMBED_SIZE = 64\n",
        "LGCN_LR = 0.005\n",
        "LGCN_BATCH_SIZE = 1024\n",
        "\n",
        "# SVD model parameters\n",
        "SVD_EPOCHS = 10\n",
        "SVD_EMBED_SIZE = 64\n",
        "\n",
        "# For deterministic results\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "5GFuU1KN-wL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies and import libraries"
      ],
      "metadata": {
        "id": "J2CRIoGKKTim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGCN\n",
        "!pip install recommenders[examples]"
      ],
      "metadata": {
        "id": "2yNGIm42JGv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet\n",
        "!pip install img2vec_pytorch"
      ],
      "metadata": {
        "id": "EBn-S6vDLMwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import math\n",
        "import statistics\n",
        "import random\n",
        "random.seed(SEED)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "seed(SEED)\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "import papermill as pm\n",
        "import scrapbook as sb\n",
        "from tensorflow.random import set_seed\n",
        "set_seed(SEED)\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Pandas version: {}\".format(pd.__version__))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))"
      ],
      "metadata": {
        "id": "x0jGaZWEKP3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dtlipn9bWXQ"
      },
      "source": [
        "### Movielens 100k dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download datasets"
      ],
      "metadata": {
        "id": "zprMfuFIJbdg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-44BmtJdx0G"
      },
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
        "!unzip ml-100k.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://csukas.org/dipterv1/ml100k-img.zip\n",
        "!unzip ml100k-img.zip"
      ],
      "metadata": {
        "id": "ojJpRLZRA84C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "# !unzip '/content/drive/MyDrive/Colab Notebooks/ml100k-img.zip'"
      ],
      "metadata": {
        "id": "VZQNBkyNawDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load movies"
      ],
      "metadata": {
        "id": "hpML6-JA2h_F"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9raYRGhOeAh8"
      },
      "source": [
        "# !head ml-100k/u.item"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr3CBT9Zec_F"
      },
      "source": [
        "def load_movies():\n",
        "  # Reading files\n",
        "  movies_dtype={\n",
        "    'id': int,\n",
        "    'title': str,\n",
        "    'release_date': str,\n",
        "    'video_release_date': str,\n",
        "    'imdb_url': str,\n",
        "    'unknown': int,\n",
        "    'Action': int,\n",
        "    'Adventure': int,\n",
        "    'Animation': int,\n",
        "    'Childrens': int,\n",
        "    'Comedy': int,\n",
        "    'Crime': int,\n",
        "    'Documentary': int,\n",
        "    'Drama': int,\n",
        "    'Fantasy': int,\n",
        "    'FilmNoir': int,\n",
        "    'Horror': int,\n",
        "    'Musical': int,\n",
        "    'Mystery': int,\n",
        "    'Romance': int,\n",
        "    'SciFi': int,\n",
        "    'Thriller': int,\n",
        "    'War': int,\n",
        "    'Western': int\n",
        "  }\n",
        "\n",
        "  names = ['id', 'title', 'release_date', 'video_release_date', 'imdb_url', 'unknown', 'Action', 'Adventure', 'Animation', 'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'FilmNoir', 'Horror', 'Musical', 'Mystery', 'Romance', 'SciFi', 'Thriller', 'War', 'Western'];\n",
        "\n",
        "  movies = pd.read_csv('ml-100k/u.item', \n",
        "                      sep='|', \n",
        "                      encoding='latin-1',\n",
        "                      dtype=movies_dtype,\n",
        "                      names=names\n",
        "                      )\n",
        "  movies.drop(['video_release_date', 'imdb_url'], axis=1, inplace=True)\n",
        "  movies.set_index('id', inplace=True)\n",
        "\n",
        "  # movies.describe(include='all')\n",
        "\n",
        "  return movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load users"
      ],
      "metadata": {
        "id": "boXu9ma02oHH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T75APGiL2oHI"
      },
      "source": [
        "# !tail ml-100k/u.user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbhf9NIn2oHI"
      },
      "source": [
        "def load_users():\n",
        "  # Reading files\n",
        "  users_dtype={\n",
        "    'id': int,\n",
        "    'age': int,\n",
        "    'gender': str,\n",
        "    'occupation': str,\n",
        "    'zip_code': str\n",
        "  }\n",
        "\n",
        "  names = ['id', 'age', 'gender', 'occupation', 'zip_code'];\n",
        "\n",
        "  users = pd.read_csv('ml-100k/u.user', \n",
        "                      sep='|', \n",
        "                      encoding='latin-1',\n",
        "                      dtype=users_dtype,\n",
        "                      names=names\n",
        "                      )\n",
        "  users.drop(['zip_code'], axis=1, inplace=True)\n",
        "  users.set_index('id', inplace=True)\n",
        "\n",
        "  return users"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load ratings"
      ],
      "metadata": {
        "id": "uRjpi5N3D-0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ratings():\n",
        "  ratings_dtype={\n",
        "    'user_id': int,\n",
        "    'item_id': int,\n",
        "    'rating': float,\n",
        "    'timestamp': int\n",
        "  }\n",
        "\n",
        "  names = ['user_id', 'item_id', 'rating', 'timestamp'];\n",
        "\n",
        "  ratings = pd.read_csv('ml-100k/u.data', \n",
        "                      sep='\\t', \n",
        "                      encoding='latin-1',\n",
        "                      dtype=ratings_dtype,\n",
        "                      names=names\n",
        "                      )\n",
        "  ratings.drop(['timestamp'], axis=1, inplace=True)\n",
        "\n",
        "  return ratings"
      ],
      "metadata": {
        "id": "WRGnvuDWEArD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature selection and preparation"
      ],
      "metadata": {
        "id": "nybS7WIZL8q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_movie_features(movies):\n",
        "  movies = movies.copy()\n",
        "  # Movie release year\n",
        "  movies['release_year']=np.NaN\n",
        "  for ind, m in movies.iterrows():\n",
        "    match = re.search('([^\\()]*).* \\((\\d*)\\)', m['title'])\n",
        "    if match:\n",
        "      movies.at[ind,'release_year'] = match.group(2)*1\n",
        "\n",
        "  # replace missing values\n",
        "  movies.loc[movies.release_year.isna(), 'release_year'] = movies.release_year.mean()\n",
        "\n",
        "  # Drop unused cols\n",
        "  movies.drop(['title', 'release_date'], axis=1, inplace=True)\n",
        "\n",
        "  return movies"
      ],
      "metadata": {
        "id": "IloHWMxoKKqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_movies_avg_ratings(movies, ratings):\n",
        "  movies['avg_rating'] = movies.apply(lambda m: ratings[ratings.item_id == m.name]['rating'].mean(), axis=1)\n",
        "  movies['avg_rating'] = movies['avg_rating'].fillna(movies['avg_rating'].mean())\n",
        "  return movies"
      ],
      "metadata": {
        "id": "gB4wDytNHSJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def prepare_user_features(users):\n",
        "  users = users.copy()\n",
        "  # One-hot encode gender and occupation\n",
        "  encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "  gender_df = pd.DataFrame(encoder.fit_transform(users[['gender']]).toarray())\n",
        "  gender_df.index = np.arange(1, len(gender_df) + 1)\n",
        "  gender_df.columns = ['gender_%s' % str(i) for i in range(len(gender_df.columns))]\n",
        "  users = users.join(gender_df).drop(['gender'], axis=1)\n",
        "\n",
        "  occupation_df = pd.DataFrame(encoder.fit_transform(users[['occupation']]).toarray())\n",
        "  occupation_df.index = np.arange(1, len(occupation_df) + 1)\n",
        "  occupation_df.columns = ['occupation_%s' % str(i) for i in range(len(occupation_df.columns))]\n",
        "  users = users.join(occupation_df).drop(['occupation'], axis=1)\n",
        "\n",
        "  return users"
      ],
      "metadata": {
        "id": "88GS3qOO4kBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_users_avg_ratings(users, ratings):\n",
        "  users['avg_rating'] = users.apply(lambda u: ratings[ratings.user_id == u.name]['rating'].mean(), axis=1)\n",
        "  users['avg_rating'] = users['avg_rating'].fillna(users['avg_rating'].mean())\n",
        "  return users"
      ],
      "metadata": {
        "id": "Im_R9P2FN4rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature normalization"
      ],
      "metadata": {
        "id": "0Yp6ihZG6-JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_df(df):\n",
        "  for c in list(df.columns):\n",
        "    c_min = df[c].min()\n",
        "    c_max = df[c].max()\n",
        "    df[c] = (df[c] - c_min) / (c_max - c_min)\n",
        "    \n",
        "  return df"
      ],
      "metadata": {
        "id": "suPlhSpe6sw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split data for crossvalidation"
      ],
      "metadata": {
        "id": "Te_HwA5qEW72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_crossval_buckets(ratings, n_folds):\n",
        "  folds = []\n",
        "  for i in range(n_folds):\n",
        "    folds.append([])\n",
        "\n",
        "  for i in range(1, (ratings['user_id'].max() + 1)):\n",
        "    u_rows = ratings.index[ratings.user_id == i].tolist()\n",
        "    for j in range(len(u_rows)):\n",
        "      folds[j % n_folds].append(u_rows[j])\n",
        "\n",
        "  return [ratings.filter(items=folds[i], axis=0).sample(frac=1) for i in range(n_folds)]"
      ],
      "metadata": {
        "id": "AMJe8RU79O9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create user and item embeddings with LightGCN"
      ],
      "metadata": {
        "id": "b0QONm6XK4e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dotdict(dict):\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__"
      ],
      "metadata": {
        "id": "CGnyu5RqQ99c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "1c8Y9dNVt3iM"
      },
      "outputs": [],
      "source": [
        "# Source https://github.com/microsoft/recommenders/blob/main/examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.models.deeprec.models.graphrec.lightgcn import LightGCN\n",
        "from recommenders.models.deeprec.DataModel.ImplicitCF import ImplicitCF\n",
        "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
        "\n",
        "def create_user_item_embeddings(train, test):\n",
        "  print('Creating user item embeddings...')\n",
        "\n",
        "  TOP_K = 10 # top k items to recommend\n",
        "  hparams = dotdict({\n",
        "    \"model_type\" : \"lightgcn\",\n",
        "    \"embed_size\" : LGCN_EMBED_SIZE, # the embedding dimension of users and items\n",
        "    \"n_layers\" : LGCN_N_LAYERS, # number of layers of the model\n",
        "    \"batch_size\" : LGCN_BATCH_SIZE,\n",
        "    \"decay\" : 0.0001, # l2 regularization for embedding parameters\n",
        "    \"epochs\" : LGCN_EPOCHS, # number of epochs for training\n",
        "    \"learning_rate\" : LGCN_LR,\n",
        "    \"eval_epoch\" : LGCN_EVAL_EPOCHS, # if it is not -1, evaluate the model every eval_epoch; -1 means that evaluation will not be performed during training\n",
        "    \"top_k\" : 10, # number of items to recommend when calculating evaluation metrics\n",
        "    \"save_model\" : False, # whether to save model\n",
        "    \"save_epoch\" : 100, # if save_model is set to True, save the model every save_epoch\n",
        "    \"metrics\" : [\"recall\", \"ndcg\", \"precision\", \"map\"], # metrics for evaluation\n",
        "    \"MODEL_DIR\" : \"./tests/resources/deeprec/lightgcn/model/lightgcn_model/\" # directory of saved models\n",
        "  })\n",
        "\n",
        "  train = train.rename(columns = {'user_id':'userID', 'item_id':'itemID'})\n",
        "  test = test.rename(columns = {'user_id':'userID', 'item_id':'itemID'})\n",
        "\n",
        "  data = ImplicitCF(train=train, test=test, seed=SEED) # col_user='user_id', col_item='item_id', col_rating='rating', \n",
        "\n",
        "  model = LightGCN(hparams, data, seed=SEED)\n",
        "  with Timer() as train_time:\n",
        "      model.fit()\n",
        "\n",
        "  print(\"Took {} seconds for training.\".format(train_time.interval))\n",
        "\n",
        "  # eval_map = map_at_k(test, topk_scores, k=TOP_K)\n",
        "  # eval_ndcg = ndcg_at_k(test, topk_scores, k=TOP_K)\n",
        "  # eval_precision = precision_at_k(test, topk_scores, k=TOP_K)\n",
        "  # eval_recall = recall_at_k(test, topk_scores, k=TOP_K)\n",
        "\n",
        "  # print(\"MAP:\\t%f\" % eval_map,\n",
        "  #       \"NDCG:\\t%f\" % eval_ndcg,\n",
        "  #       \"Precision@K:\\t%f\" % eval_precision,\n",
        "  #       \"Recall@K:\\t%f\" % eval_recall, sep='\\n')\n",
        "\n",
        "  # user embeddings\n",
        "  user_embeddings = [[] for i in range(train['userID'].max()+1)]\n",
        "  embeddings = list(model.ua_embeddings.eval(session=model.sess))\n",
        "  for i in range(model.n_users):\n",
        "    user_embeddings[model.data.id2user[i]] = embeddings[i]\n",
        "\n",
        "  movie_embeddings = [[] for i in range(train['itemID'].max()+1)]\n",
        "  embeddings = list(model.ia_embeddings.eval(session=model.sess))\n",
        "  for i in range(model.n_items):\n",
        "    movie_embeddings[model.data.id2item[i]] = embeddings[i]\n",
        "  \n",
        "  print('User item embeddings are created.')\n",
        "\n",
        "  return (user_embeddings, movie_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create image embeddings with ResNet"
      ],
      "metadata": {
        "id": "DVhRMe-2LVtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from img2vec_pytorch import Img2Vec\n",
        "from PIL import Image\n",
        "\n",
        "def create_image_embeddings(img_df):\n",
        "  print('Creating image embeddings...')\n",
        "\n",
        "  if os.path.isfile('img_embeddings.npy'):\n",
        "    with open('img_embeddings.npy', 'rb') as f:\n",
        "      embeddings = np.load(f, allow_pickle=True)\n",
        "\n",
        "    print('Image embeddings are already created.')\n",
        "      \n",
        "    return embeddings\n",
        "\n",
        "  embeddings = [[] for i in range(img_df['id'].max()+1)]\n",
        "\n",
        "  img2vec = Img2Vec(cuda=True)\n",
        "\n",
        "  for ind, img in img_df.iterrows():\n",
        "    if not os.path.isfile(img['file']):\n",
        "      embeddings[img['id']] = [0 for i in range(512)]\n",
        "      continue\n",
        "\n",
        "    print(img['file'])\n",
        "    \n",
        "    imgf = Image.open(img['file']).convert('RGB')\n",
        "    embeddings[img['id']] = img2vec.get_vec(imgf)\n",
        "\n",
        "  embeddings = np.array(embeddings)\n",
        "  with open('img_embeddings.npy', 'wb') as f:\n",
        "    np.save(f, embeddings)\n",
        "\n",
        "  print('Image embeddings are created.')\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "5ffyzu8yd22A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rotate array by d elements using temp array"
      ],
      "metadata": {
        "id": "3GrmPi7gkiCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source https://www.geeksforgeeks.org/python-program-for-program-for-array-rotation-2/\n",
        "# function to rotate array by d elements using temp array\n",
        "def rotateArray(arr, n, d):\n",
        "    temp = []\n",
        "    i = 0\n",
        "    while (i < d):\n",
        "        temp.append(arr[i])\n",
        "        i = i + 1\n",
        "    i = 0\n",
        "    while (d < n):\n",
        "        arr[i] = arr[d]\n",
        "        i = i + 1\n",
        "        d = d + 1\n",
        "    arr[:] = arr[: i] + temp\n",
        "    return arr"
      ],
      "metadata": {
        "id": "nj-jfDQikczn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "FIsxYPQBktxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import SVD\n",
        "\n",
        "def build_SVD_model():\n",
        "  return SVD(verbose=True, n_factors=SVD_EMBED_SIZE, n_epochs=SVD_EPOCHS)"
      ],
      "metadata": {
        "id": "-8uCw7WHrSfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "def build_NeuMF_model(ncf_layers):\n",
        "  movie_features_input = keras.Input(\n",
        "    shape=(21,), name=\"movie_features\"\n",
        "  )\n",
        "  movie_poster_embeddings_input = keras.Input(\n",
        "    shape=(512,), name=\"movie_poster_embeddings\"\n",
        "  )\n",
        "  movie_embeddings_input = keras.Input(\n",
        "    shape=(LGCN_EMBED_SIZE,), name=\"movie_embeddings\"\n",
        "  )\n",
        "  user_embeddings_input = keras.Input(\n",
        "    shape=(LGCN_EMBED_SIZE,), name=\"user_embeddings\"\n",
        "  )\n",
        "  user_features_input = keras.Input(\n",
        "    shape=(25,), name=\"user_features\"\n",
        "  )\n",
        "\n",
        "  gmf_layer = keras.layers.Multiply(name=\"gmf_layer\")([movie_embeddings_input, user_embeddings_input])\n",
        "\n",
        "  last_ncf_layer = layers.concatenate([movie_features_input, movie_poster_embeddings_input, movie_embeddings_input, user_embeddings_input, user_features_input])\n",
        "  for i in range(len(ncf_layers)):\n",
        "    last_ncf_layer = layers.Dense(ncf_layers[i], activation=\"relu\")(last_ncf_layer)\n",
        "    # last_ncf_layer = layers.Dropout(NCF_DROPOUT, noise_shape=None, seed=SEED)(last_ncf_layer)\n",
        "    last_ncf_layer = layers.BatchNormalization()(last_ncf_layer)\n",
        "\n",
        "  concatenate_layer = layers.concatenate([gmf_layer, last_ncf_layer])\n",
        "  \n",
        "  output_layer = layers.Dense(1, activation=\"relu\")(concatenate_layer)\n",
        "\n",
        "  model = keras.Model(\n",
        "    inputs=[movie_features_input, movie_poster_embeddings_input, movie_embeddings_input, user_embeddings_input, user_features_input],\n",
        "    outputs=output_layer,\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "g0coqj4JlJY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crossvalidation"
      ],
      "metadata": {
        "id": "hkX0pAIHkpmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def crossvalidate_model(model, train_fn, predict_fn, movies, users, cv_buckets, n_train):\n",
        "  rmse = []\n",
        "  mae = []\n",
        "  for i in range(len(cv_buckets)):\n",
        "    print('Cross-validation round: %s' % (str(i+1)))\n",
        "    # sort out data from buckets to splits\n",
        "    cv_buckets = rotateArray(cv_buckets, len(cv_buckets), 1)\n",
        "    # train\n",
        "    train_d = []\n",
        "    for i in range(0, n_train):\n",
        "      train_d.append(cv_buckets[i])\n",
        "    train_df = pd.concat(train_d)\n",
        "    # test\n",
        "    test_d = []\n",
        "    for i in range(n_train, len(cv_buckets)):\n",
        "      test_d.append(cv_buckets[i])\n",
        "    test_df = pd.concat(test_d)\n",
        "\n",
        "    # train model\n",
        "    trained_model = train_fn(model, movies, users, train_df)\n",
        "\n",
        "    # predict\n",
        "    predictions = predict_fn(trained_model, test_df)\n",
        "\n",
        "    y_actual = test_df['rating'].to_list()\n",
        "\n",
        "    c_rmse = mean_squared_error(y_actual, predictions, squared=False)\n",
        "    rmse.append(c_rmse)\n",
        "    c_mae = mean_absolute_error(y_actual, predictions)\n",
        "    mae.append(c_mae)\n",
        "\n",
        "    print(\"End of cross-validation round. RMSE=%s, MAE=%s | AVG(RMSE)=%s, AVG(MAE)=%s.\" % (str(c_rmse), str(c_mae), str(statistics.mean(rmse)), str(statistics.mean(mae))))\n",
        "\n",
        "  return (rmse, mae)"
      ],
      "metadata": {
        "id": "FpAJR1u8Qkkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AVG train & predict"
      ],
      "metadata": {
        "id": "nmIZPDF2ZcEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "\n",
        "def train_avg(model, movies_df, users_df, train_df):\n",
        "  movies_df = movies_df.copy()\n",
        "  movies_df = prepare_movies_avg_ratings(movies_df, train_df)[['avg_rating']]\n",
        "  movies_df['avg_rating'] = movies_df['avg_rating'].fillna(movies_df['avg_rating'].mean())\n",
        "\n",
        "  return movies_df"
      ],
      "metadata": {
        "id": "x2DWLTZTZcEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_avg(model, test_df):\n",
        "  predictions = []\n",
        "  for ind, r in test_df.iterrows():\n",
        "    predictions.append(model.loc[r['item_id']]['avg_rating'])\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "l39vafFYZcEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SVD train & predict"
      ],
      "metadata": {
        "id": "qXGN_3ec4CiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "\n",
        "def train_svd(model, movies_df, users_df, train_df):\n",
        "  reader = Reader(rating_scale=(1, 5))\n",
        "  data = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)\n",
        "  trainset = data.build_full_trainset()\n",
        "\n",
        "  model.fit(trainset)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "4OOLWLaKlgz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_svd(model, test_df):\n",
        "  predictions = []\n",
        "  for ind, r in test_df.iterrows():\n",
        "    predictions.append(model.predict(uid=r['user_id'], iid=r['item_id']).est)\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "mQ4brXxjlg0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NeuMF train & predict"
      ],
      "metadata": {
        "id": "M-fZ9Ipe4G98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def train_ncf(model, movies_df, users_df, train_df):\n",
        "  movies_df = normalize_df(prepare_movies_avg_ratings(prepare_movie_features(movies_df), train_df))\n",
        "  users_df = normalize_df(prepare_users_avg_ratings(prepare_user_features(users_df), train_df))\n",
        "\n",
        "  # create image embeddings for movie posters\n",
        "  img_df = movies_df.reset_index()[['id']]\n",
        "  img_df['id_str'] = img_df['id'].apply(str)\n",
        "  img_df['file'] = './img/' + img_df['id_str'] + '.jpg'\n",
        "  img_df.drop(['id_str'], axis=1, inplace=True)\n",
        "\n",
        "  movie_poster_embeddings = create_image_embeddings(img_df)\n",
        "    \n",
        "  # create user and movie embeddings\n",
        "  # create dummy ratings for users and movies with unobserved ratings\n",
        "  next_movie_id = movies_df.index.max() + 1\n",
        "  next_user_id = users_df.index.max() + 1\n",
        "  avg_rating = round(train_df.rating.mean())\n",
        "  for ind, m in movies_df.iterrows():\n",
        "    if len(train_df[train_df.item_id == ind]) == 0:\n",
        "      train_df = train_df.append({'user_id': next_user_id, 'item_id': ind, 'rating': avg_rating}, ignore_index=True)\n",
        "  for ind, u in users_df.iterrows():\n",
        "    if len(train_df[train_df.user_id == ind]) == 0:\n",
        "      train_df = train_df.append({'user_id': ind, 'item_id': next_movie_id, 'rating': avg_rating}, ignore_index=True)\n",
        "  # create embeddings\n",
        "  user_embeddings, movie_embeddings = create_user_item_embeddings(train_df, train_df)\n",
        "  # delete dummy ratings\n",
        "  train_df = train_df[(train_df.item_id != next_movie_id) & (train_df.user_id != next_user_id)]\n",
        "\n",
        "  x_mf = []\n",
        "  x_mie = []\n",
        "  x_me = []\n",
        "  x_ue = []\n",
        "  x_uf = []\n",
        "  for i, r in train_df.iterrows():\n",
        "    x_mf.append(movies_df.loc[int(r['item_id'])].to_list())\n",
        "    x_mie.append(movie_poster_embeddings[int(r['item_id'])])\n",
        "    x_me.append(movie_embeddings[int(r['item_id'])])\n",
        "    x_ue.append(user_embeddings[int(r['user_id'])])\n",
        "    x_uf.append(users_df.loc[int(r['user_id'])].to_list())\n",
        "  x_mf = np.array(x_mf)\n",
        "  x_mie = np.array(x_mie)\n",
        "  x_me = np.array(x_me)\n",
        "  x_ue = np.array(x_ue)\n",
        "  x_uf = np.array(x_uf)\n",
        "\n",
        "  y_train = np.array(train_df['rating'].to_list())\n",
        "\n",
        "  model_copy = keras.models.clone_model(model)\n",
        "\n",
        "  opt = keras.optimizers.Adam(learning_rate=NCF_LR)\n",
        "\n",
        "  model_copy.compile(optimizer=opt,\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=NCF_ES_PATIENCE)\n",
        "  mc = ModelCheckpoint('ncf_model', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "\n",
        "  H = model_copy.fit(\n",
        "    {\"movie_features\": x_mf, \"movie_poster_embeddings\": x_mie, \"movie_embeddings\": x_me, \"user_embeddings\": x_ue, \"user_features\": x_uf},\n",
        "    y_train, \n",
        "    validation_split=0.2,\n",
        "    batch_size=NCF_BATCH_SIZE,\n",
        "    epochs=NCF_EPOCHS,\n",
        "    verbose=1,  \n",
        "    callbacks=[es, mc]\n",
        "  )\n",
        "\n",
        "  return (keras.models.load_model('ncf_model'), movies_df, users_df, movie_embeddings, movie_poster_embeddings, user_embeddings)"
      ],
      "metadata": {
        "id": "8vVnxYWN3iPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ncf(model, test_df):\n",
        "  nn_model, movies_df, users_df, movie_embeddings, movie_poster_embeddings, user_embeddings = model\n",
        "\n",
        "  x_mf = []\n",
        "  x_mie = []\n",
        "  x_me = []\n",
        "  x_ue = []\n",
        "  x_uf = []\n",
        "  for i, r in test_df.iterrows():\n",
        "    x_mf.append(movies_df.loc[int(r['item_id'])].to_list())\n",
        "    x_mie.append(movie_poster_embeddings[int(r['item_id'])])\n",
        "    x_me.append(movie_embeddings[int(r['item_id'])])\n",
        "    x_ue.append(user_embeddings[int(r['user_id'])])\n",
        "    x_uf.append(users_df.loc[int(r['user_id'])].to_list())\n",
        "  x_mf = np.array(x_mf)\n",
        "  x_mie = np.array(x_mie)\n",
        "  x_me = np.array(x_me)\n",
        "  x_ue = np.array(x_ue)\n",
        "  x_uf = np.array(x_uf)\n",
        "\n",
        "  # print(mids[np.apply_along_axis(lambda a: np.count_nonzero(a) > 0, axis=1, arr=np.isnan(x_mf))])\n",
        "\n",
        "  return nn_model.predict({\"movie_features\": x_mf, \"movie_poster_embeddings\": x_mie, \"movie_embeddings\": x_me, \"user_embeddings\": x_ue, \"user_features\": x_uf})"
      ],
      "metadata": {
        "id": "O_uQYCu2-5C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run crossvalidation"
      ],
      "metadata": {
        "id": "tdPZoxkbWROV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load datasets\n",
        "movies_df = load_movies()\n",
        "users_df = load_users()\n",
        "ratings_df = load_ratings()\n",
        "\n",
        "# prepare for crossvalidation\n",
        "cv_buckets = create_crossval_buckets(ratings_df, CV_NUM_FOLDS)"
      ],
      "metadata": {
        "id": "2u-tLiZorG-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### AVG"
      ],
      "metadata": {
        "id": "Yyo_nF7w1yP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse, mae = crossvalidate_model(None, train_avg, predict_avg, movies_df, users_df, cv_buckets, n_train=math.floor(CV_NUM_FOLDS * CV_TRAIN_RATIO))\n",
        "\n",
        "print(statistics.mean(rmse))\n",
        "print(statistics.mean(mae))\n",
        "\n",
        "print(rmse)\n",
        "print(mae)"
      ],
      "metadata": {
        "id": "kq_zRJkS1yP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SVD"
      ],
      "metadata": {
        "id": "BCHqJvM4WUoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_SVD_model()\n",
        "\n",
        "rmse, mae = crossvalidate_model(model, train_svd, predict_svd, movies_df, users_df, cv_buckets, n_train=math.floor(CV_NUM_FOLDS * CV_TRAIN_RATIO))\n",
        "\n",
        "print(statistics.mean(rmse))\n",
        "print(statistics.mean(mae))\n",
        "\n",
        "print(rmse)\n",
        "print(mae)"
      ],
      "metadata": {
        "id": "WOg1YZciroYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### NCF"
      ],
      "metadata": {
        "id": "EKsjhgscWXzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_NeuMF_model(ncf_layers=[686, 1024, 512, 128, 32])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "rmse, mae = crossvalidate_model(model, train_ncf, predict_ncf, movies_df, users_df, cv_buckets, n_train=math.floor(CV_NUM_FOLDS * CV_TRAIN_RATIO))\n",
        "\n",
        "print(statistics.mean(rmse))\n",
        "print(statistics.mean(mae))\n",
        "\n",
        "print(rmse)\n",
        "print(mae)"
      ],
      "metadata": {
        "id": "8ChhfAbS-p7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9oox8h5XeoV6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}